{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jryJaNsP24FR",
        "outputId": "e460bcb3-1ea9-419f-fac9-68de2847ff1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,659 kB]\n",
            "Hit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,696 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,230 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,657 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,634 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,939 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,526 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Fetched 23.8 MB in 3s (8,470 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "--2025-02-24 23:23:25--  https://downloads.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.208.237, 2a01:4f8:10a:39da::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400879762 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.4-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.4-bin-had 100%[===================>] 382.31M  24.1MB/s    in 17s     \n",
            "\n",
            "2025-02-24 23:23:43 (22.4 MB/s) - ‘spark-3.5.4-bin-hadoop3.tgz’ saved [400879762/400879762]\n",
            "\n",
            "Collecting pyspark==3.5.0\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pydeequ\n",
            "  Downloading pydeequ-1.4.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting faker\n",
            "  Downloading Faker-36.1.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting py4j==0.10.9.7 (from pyspark==3.5.0)\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from pydeequ) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeequ-1.4.0-py3-none-any.whl (37 kB)\n",
            "Downloading Faker-36.1.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425346 sha256=d9c258afb328fb6f258c84ab1cac3e401a652f44eb369a9cbd9b4c91e1266d39\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/df/61/8c121f50c3cffd77f8178180dd232d90b3b99d1bd61fb6d6be\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark, faker, pydeequ\n",
            "Successfully installed faker-36.1.1 py4j-0.10.9.7 pydeequ-1.4.0 pyspark-3.5.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  sqlite3-doc\n",
            "The following NEW packages will be installed:\n",
            "  sqlite3\n",
            "0 upgraded, 1 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 768 kB of archives.\n",
            "After this operation, 1,873 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.3 [768 kB]\n",
            "Fetched 768 kB in 1s (741 kB/s)\n",
            "Selecting previously unselected package sqlite3.\n",
            "(Reading database ... 120778 files and directories currently installed.)\n",
            "Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking sqlite3 (3.37.2-2ubuntu0.3) ...\n",
            "Setting up sqlite3 (3.37.2-2ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "--2025-02-24 23:24:41--  https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.42.0.0/sqlite-jdbc-3.42.0.0.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13030515 (12M) [application/java-archive]\n",
            "Saving to: ‘/content/sqlite-jdbc-3.42.0.0.jar’\n",
            "\n",
            "/content/sqlite-jdb 100%[===================>]  12.43M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-02-24 23:24:42 (104 MB/s) - ‘/content/sqlite-jdbc-3.42.0.0.jar’ saved [13030515/13030515]\n",
            "\n",
            "--2025-02-24 23:24:42--  https://repo1.maven.org/maven2/com/amazon/deequ/deequ/2.0.7-spark-3.5/deequ-2.0.7-spark-3.5.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1538311 (1.5M) [application/java-archive]\n",
            "Saving to: ‘/content/deequ-2.0.7-spark-3.5.jar’\n",
            "\n",
            "/content/deequ-2.0. 100%[===================>]   1.47M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-02-24 23:24:42 (20.8 MB/s) - ‘/content/deequ-2.0.7-spark-3.5.jar’ saved [1538311/1538311]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install Java 11\n",
        "!apt-get update\n",
        "!apt-get install -y openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download Spark 3.5.4 with Hadoop 3.3\n",
        "!wget https://downloads.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "# Set environment variables for Java and Spark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\"\n",
        "os.environ[\"SPARK_VERSION\"] = \"3.5\"\n",
        "\n",
        "# Install PySpark 3.5.0, PyDeequ, Pandas and SQLite\n",
        "!pip install pyspark==3.5.0 pydeequ pandas faker\n",
        "!apt-get install sqlite3\n",
        "\n",
        "# Download SQLite JDBC driver\n",
        "!wget https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.42.0.0/sqlite-jdbc-3.42.0.0.jar -O /content/sqlite-jdbc-3.42.0.0.jar\n",
        "\n",
        "# Download Deequ 2.0.7 JAR for Spark 3.5\n",
        "!wget https://repo1.maven.org/maven2/com/amazon/deequ/deequ/2.0.7-spark-3.5/deequ-2.0.7-spark-3.5.jar -O /content/deequ-2.0.7-spark-3.5.jar\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabulate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBHapAcD3JZU",
        "outputId": "0000a1d1-d0a5-4a70-92c2-43180297c29d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tabulate\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: tabulate\n",
            "Successfully installed tabulate-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import mean, min, max, col, count\n",
        "from pydeequ.analyzers import AnalysisRunner, AnalyzerContext\n",
        "from pydeequ.analyzers import Size, Completeness, Mean, Maximum, Minimum, CountDistinct, Distinctness\n",
        "from pydeequ.checks import Check, CheckLevel\n",
        "from pydeequ.verification import VerificationSuite, VerificationResult\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import json\n",
        "from tabulate import tabulate\n",
        "import sqlite3\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('data_quality_validation.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def create_spark_session():\n",
        "    \"\"\"Initialize Spark session with required configurations\"\"\"\n",
        "    logger.info(\"Initializing Spark session...\")\n",
        "    try:\n",
        "        spark = (SparkSession.builder\n",
        "            .config(\"spark.jars\", \"/content/sqlite-jdbc-3.42.0.0.jar,/content/deequ-2.0.7-spark-3.5.jar\")\n",
        "            .config(\"spark.driver.memory\", \"16g\")\n",
        "            .getOrCreate())\n",
        "        logger.info(\"Spark session created successfully\")\n",
        "        return spark\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to create Spark session: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def load_dataframes(spark):\n",
        "    \"\"\"Load CSV files into Spark DataFrames with error handling\"\"\"\n",
        "    logger.info(\"Loading data from CSV files...\")\n",
        "    try:\n",
        "        patients_df = spark.read.csv('patients.csv', header=True, inferSchema=True)\n",
        "        encounters_df = spark.read.csv('encounters.csv', header=True, inferSchema=True)\n",
        "        procedures_df = spark.read.csv('procedures.csv', header=True, inferSchema=True)\n",
        "\n",
        "        # Log basic statistics\n",
        "        logger.info(f\"Patients dataset loaded: {patients_df.count()} records\")\n",
        "        logger.info(f\"Encounters dataset loaded: {encounters_df.count()} records\")\n",
        "        logger.info(f\"Procedures dataset loaded: {procedures_df.count()} records\")\n",
        "\n",
        "        return patients_df, encounters_df, procedures_df\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def verify_dataset(spark, df, dataset_name):\n",
        "    \"\"\"Run verification checks on a dataset using PyDeequ with enhanced details\"\"\"\n",
        "    logger.info(f\"Verifying {dataset_name} dataset using PyDeequ...\")\n",
        "\n",
        "    # Dataset-specific ID field\n",
        "    id_field = \"Id\" if dataset_name == \"patients\" else \"PATIENT\"\n",
        "\n",
        "    # Create a check with the spark session included\n",
        "    check = Check(spark, CheckLevel.Error, f\"{dataset_name} Data Quality Checks\")\n",
        "\n",
        "    # Base checks for all datasets\n",
        "    base_check = check.hasSize(lambda x: x > 0) \\\n",
        "                      .isComplete(id_field)\n",
        "\n",
        "    # Dataset-specific checks\n",
        "    if dataset_name == \"patients\":\n",
        "        # Add patient-specific checks\n",
        "        verification_check = base_check \\\n",
        "            .isContainedIn(\"GENDER\", [\"M\", \"F\"]) \\\n",
        "            .isComplete(\"BIRTHDATE\")\n",
        "\n",
        "    elif dataset_name == \"encounters\":\n",
        "        # Add encounter-specific checks\n",
        "        verification_check = base_check \\\n",
        "            .isNonNegative(\"BASE_ENCOUNTER_COST\") \\\n",
        "            .isComplete(\"START\") \\\n",
        "            .isComplete(\"STOP\")\n",
        "\n",
        "    elif dataset_name == \"procedures\":\n",
        "        # Add procedure-specific checks\n",
        "        verification_check = base_check \\\n",
        "            .isNonNegative(\"BASE_COST\") \\\n",
        "            .isComplete(\"DESCRIPTION\") \\\n",
        "            .isComplete(\"START\") \\\n",
        "            .isComplete(\"STOP\")\n",
        "\n",
        "    # Run the verification suite with the constructed check\n",
        "    verification_result = VerificationSuite(spark) \\\n",
        "        .onData(df) \\\n",
        "        .addCheck(verification_check) \\\n",
        "        .run()\n",
        "\n",
        "    # Convert the results to a DataFrame\n",
        "    result_df = VerificationResult.checkResultsAsDataFrame(spark, verification_result)\n",
        "\n",
        "    # Process verification results\n",
        "    verification_results = []\n",
        "\n",
        "    for row in result_df.collect():\n",
        "        status = \"Success\" if row[\"constraint_status\"] == \"Success\" else \"Error\"\n",
        "        constraint = row[\"constraint\"]\n",
        "\n",
        "        # Enhance the details column with meaningful information based on the constraint type\n",
        "        details = row[\"constraint_message\"] if row[\"constraint_message\"] else \"\"\n",
        "\n",
        "        if status == \"Success\" and not details:\n",
        "            # Add appropriate success messages based on constraint type\n",
        "            if \"SizeConstraint\" in constraint:\n",
        "                record_count = df.count()\n",
        "                details = f\"Dataset contains {record_count:,} records, which meets the size requirement.\"\n",
        "\n",
        "            elif \"CompletenessConstraint\" in constraint:\n",
        "                # Extract column name from constraint description using safer regex\n",
        "                import re\n",
        "                match = re.search(r\"Completeness\\(([^,]+)\", constraint)\n",
        "                if match:\n",
        "                    column_name = match.group(1)\n",
        "                    # Only proceed if the column name is actually in the dataframe\n",
        "                    if column_name in df.columns:\n",
        "                        completeness = df.filter(df[column_name].isNotNull()).count() / float(df.count()) * 100\n",
        "                        details = f\"Column '{column_name}' is {completeness:.2f}% complete.\"\n",
        "                    else:\n",
        "                        details = f\"Column completeness check passed for {column_name}.\"\n",
        "                else:\n",
        "                    details = f\"Column completeness check passed.\"\n",
        "\n",
        "            elif \"ComplianceConstraint\" in constraint and \"contained in\" in constraint:\n",
        "                # For gender validation\n",
        "                gender_counts = df.groupBy(\"GENDER\").count().collect()\n",
        "                gender_info = \", \".join([f\"{row['GENDER']}: {row['count']:,}\" for row in gender_counts])\n",
        "                details = f\"Gender distribution: {gender_info}\"\n",
        "\n",
        "            elif \"ComplianceConstraint\" in constraint and \"non-negative\" in constraint:\n",
        "                # Extract cost column from the constraint\n",
        "                if \"BASE_ENCOUNTER_COST\" in constraint:\n",
        "                    cost_col = \"BASE_ENCOUNTER_COST\"\n",
        "                elif \"BASE_COST\" in constraint:\n",
        "                    cost_col = \"BASE_COST\"\n",
        "                else:\n",
        "                    cost_col = None\n",
        "\n",
        "                if cost_col and cost_col in df.columns:\n",
        "                    cost_stats = df.select(\n",
        "                        mean(cost_col).alias(\"mean\"),\n",
        "                        min(cost_col).alias(\"min\"),\n",
        "                        max(cost_col).alias(\"max\")\n",
        "                    ).collect()[0]\n",
        "\n",
        "                    cost_type = \"encounter\" if cost_col == \"BASE_ENCOUNTER_COST\" else \"procedure\"\n",
        "                    details = f\"All {cost_type} costs are non-negative. Min: ${cost_stats['min']:.2f}, Max: ${cost_stats['max']:.2f}, Mean: ${cost_stats['mean']:.2f}\"\n",
        "                else:\n",
        "                    details = \"All values are non-negative.\"\n",
        "            else:\n",
        "                details = \"Check passed successfully.\"\n",
        "\n",
        "        verification_results.append({\n",
        "            \"check_description\": constraint,\n",
        "            \"status\": status,\n",
        "            \"details\": details\n",
        "        })\n",
        "\n",
        "    return verification_results\n",
        "\n",
        "def analyze_dataset(spark, df, dataset_name):\n",
        "    \"\"\"Analyze a single dataset using PyDeequ analyzers\"\"\"\n",
        "    logger.info(f\"Analyzing {dataset_name} dataset using PyDeequ...\")\n",
        "    try:\n",
        "        # Initialize analysis runners for different metrics\n",
        "        analysis_results = []\n",
        "\n",
        "        # Basic size analysis\n",
        "        size_analyzer = AnalysisRunner(spark) \\\n",
        "            .onData(df) \\\n",
        "            .addAnalyzer(Size()) \\\n",
        "            .run()\n",
        "\n",
        "        size_metrics = AnalyzerContext.successMetricsAsDataFrame(spark, size_analyzer)\n",
        "\n",
        "        # Extract row count\n",
        "        for row in size_metrics.collect():\n",
        "            if row[\"name\"] == \"Size\":\n",
        "                analysis_results.append({\n",
        "                    \"analyzer\": \"Record Count\",\n",
        "                    \"value\": row[\"value\"]\n",
        "                })\n",
        "\n",
        "        # Column completeness analysis\n",
        "        completeness_analyzer = AnalysisRunner(spark) \\\n",
        "            .onData(df) \\\n",
        "            .addAnalyzer(Completeness(\"*\")) \\\n",
        "            .run()\n",
        "\n",
        "        completeness_metrics = AnalyzerContext.successMetricsAsDataFrame(spark, completeness_analyzer)\n",
        "\n",
        "        # Extract completeness metrics\n",
        "        for row in completeness_metrics.collect():\n",
        "            if row[\"name\"] == \"Completeness\":\n",
        "                column_name = row[\"instance\"]\n",
        "                analysis_results.append({\n",
        "                    \"analyzer\": f\"Completeness ({column_name})\",\n",
        "                    \"value\": row[\"value\"]\n",
        "                })\n",
        "\n",
        "        # Dataset-specific analysis\n",
        "        if dataset_name == \"patients\":\n",
        "            # Gender distribution - using direct Spark operations\n",
        "            gender_dist = df.groupBy(\"GENDER\").count().collect()\n",
        "            row_count = df.count()\n",
        "\n",
        "            for row in gender_dist:\n",
        "                analysis_results.append({\n",
        "                    \"analyzer\": f\"Gender Distribution ({row['GENDER']})\",\n",
        "                    \"value\": row['count'] / float(row_count)\n",
        "                })\n",
        "\n",
        "        elif dataset_name == \"encounters\":\n",
        "            # Cost statistics\n",
        "            cost_col = \"BASE_ENCOUNTER_COST\"\n",
        "\n",
        "            cost_analyzer = AnalysisRunner(spark) \\\n",
        "                .onData(df) \\\n",
        "                .addAnalyzer(Minimum(cost_col)) \\\n",
        "                .addAnalyzer(Maximum(cost_col)) \\\n",
        "                .addAnalyzer(Mean(cost_col)) \\\n",
        "                .run()\n",
        "\n",
        "            cost_metrics = AnalyzerContext.successMetricsAsDataFrame(spark, cost_analyzer)\n",
        "\n",
        "            # Extract cost metrics\n",
        "            for row in cost_metrics.collect():\n",
        "                if row[\"name\"] == \"Minimum\" and row[\"instance\"] == cost_col:\n",
        "                    analysis_results.append({\n",
        "                        \"analyzer\": \"Min Encounter Cost\",\n",
        "                        \"value\": float(row[\"value\"])\n",
        "                    })\n",
        "                elif row[\"name\"] == \"Maximum\" and row[\"instance\"] == cost_col:\n",
        "                    analysis_results.append({\n",
        "                        \"analyzer\": \"Max Encounter Cost\",\n",
        "                        \"value\": float(row[\"value\"])\n",
        "                    })\n",
        "                elif row[\"name\"] == \"Mean\" and row[\"instance\"] == cost_col:\n",
        "                    analysis_results.append({\n",
        "                        \"analyzer\": \"Mean Encounter Cost\",\n",
        "                        \"value\": float(row[\"value\"])\n",
        "                    })\n",
        "\n",
        "        elif dataset_name == \"procedures\":\n",
        "            # Cost statistics\n",
        "            cost_col = \"BASE_COST\"\n",
        "\n",
        "            cost_analyzer = AnalysisRunner(spark) \\\n",
        "                .onData(df) \\\n",
        "                .addAnalyzer(Minimum(cost_col)) \\\n",
        "                .addAnalyzer(Maximum(cost_col)) \\\n",
        "                .addAnalyzer(Mean(cost_col)) \\\n",
        "                .run()\n",
        "\n",
        "            cost_metrics = AnalyzerContext.successMetricsAsDataFrame(spark, cost_analyzer)\n",
        "\n",
        "            # Extract cost metrics\n",
        "            for row in cost_metrics.collect():\n",
        "                if row[\"name\"] == \"Minimum\" and row[\"instance\"] == cost_col:\n",
        "                    analysis_results.append({\n",
        "                        \"analyzer\": \"Min Procedure Cost\",\n",
        "                        \"value\": float(row[\"value\"])\n",
        "                    })\n",
        "                elif row[\"name\"] == \"Maximum\" and row[\"instance\"] == cost_col:\n",
        "                    analysis_results.append({\n",
        "                        \"analyzer\": \"Max Procedure Cost\",\n",
        "                        \"value\": float(row[\"value\"])\n",
        "                    })\n",
        "                elif row[\"name\"] == \"Mean\" and row[\"instance\"] == cost_col:\n",
        "                    analysis_results.append({\n",
        "                        \"analyzer\": \"Mean Procedure Cost\",\n",
        "                        \"value\": float(row[\"value\"])\n",
        "                    })\n",
        "\n",
        "        return analysis_results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error analyzing {dataset_name} dataset with PyDeequ: {str(e)}\")\n",
        "        logger.exception(\"Stack trace:\")\n",
        "        raise\n",
        "\n",
        "def save_to_sqlite(df, table_name, db_path):\n",
        "    \"\"\"Save DataFrame to SQLite with logging\"\"\"\n",
        "    try:\n",
        "        pandas_df = df.toPandas()\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        pandas_df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
        "        conn.close()\n",
        "        logger.info(f\"Successfully saved {table_name} to SQLite\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving {table_name} to SQLite: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def print_results_summary(results):\n",
        "    \"\"\"Print formatted results summary\"\"\"\n",
        "    print(\"\\n=== DATA QUALITY VALIDATION SUMMARY ===\\n\")\n",
        "\n",
        "    # Print dataset metrics\n",
        "    print(\"Dataset Metrics:\")\n",
        "    metrics_table = []\n",
        "    for dataset, metrics in results[\"dataset_metrics\"].items():\n",
        "        for metric_name, value in metrics.items():\n",
        "            metrics_table.append([dataset, metric_name, value])\n",
        "    print(tabulate(metrics_table,\n",
        "                  headers=[\"Dataset\", \"Metric\", \"Value\"],\n",
        "                  tablefmt=\"grid\"))\n",
        "\n",
        "    # Print analysis results\n",
        "    print(\"\\nAnalysis Results:\")\n",
        "    analysis_table = []\n",
        "    for dataset, analyses in results[\"analysis_results\"].items():\n",
        "        for analysis in analyses:\n",
        "            value = analysis['value']\n",
        "            formatted_value = f\"{value:.2%}\" if isinstance(value, float) and value <= 1.0 else f\"{value:,.2f}\"\n",
        "            analysis_table.append([\n",
        "                dataset,\n",
        "                analysis[\"analyzer\"],\n",
        "                formatted_value\n",
        "            ])\n",
        "    print(tabulate(analysis_table,\n",
        "                  headers=[\"Dataset\", \"Analyzer\", \"Value\"],\n",
        "                  tablefmt=\"grid\"))\n",
        "\n",
        "    # Print verification results\n",
        "    print(\"\\nVerification Results:\")\n",
        "    verification_table = []\n",
        "    for dataset, verifications in results[\"verification_results\"].items():\n",
        "        for verification in verifications:\n",
        "            verification_table.append([\n",
        "                dataset,\n",
        "                verification[\"check_description\"],\n",
        "                verification[\"status\"],\n",
        "                verification[\"details\"]\n",
        "            ])\n",
        "    print(tabulate(verification_table,\n",
        "                  headers=[\"Dataset\", \"Check Description\", \"Status\", \"Details\"],\n",
        "                  tablefmt=\"grid\"))\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    try:\n",
        "        logger.info(\"Starting data quality validation process\")\n",
        "\n",
        "        # Initialize Spark\n",
        "        spark = create_spark_session()\n",
        "\n",
        "        # Load data\n",
        "        patients_df, encounters_df, procedures_df = load_dataframes(spark)\n",
        "\n",
        "        # Initialize results dictionary\n",
        "        results = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"dataset_metrics\": {},\n",
        "            \"analysis_results\": {},\n",
        "            \"verification_results\": {}\n",
        "        }\n",
        "\n",
        "        # Process each dataset\n",
        "        datasets = {\n",
        "            \"patients\": patients_df,\n",
        "            \"encounters\": encounters_df,\n",
        "            \"procedures\": procedures_df\n",
        "        }\n",
        "\n",
        "        for dataset_name, df in datasets.items():\n",
        "            # Add basic metrics\n",
        "            results[\"dataset_metrics\"][dataset_name] = {\n",
        "                \"record_count\": df.count(),\n",
        "                \"column_count\": len(df.columns)\n",
        "            }\n",
        "\n",
        "            # Run analysis\n",
        "            results[\"analysis_results\"][dataset_name] = analyze_dataset(spark, df, dataset_name)\n",
        "\n",
        "            # Run verification\n",
        "            results[\"verification_results\"][dataset_name] = verify_dataset(spark, df, dataset_name)\n",
        "\n",
        "        # Save results to JSON\n",
        "        with open('data_quality_results.json', 'w') as f:\n",
        "            json.dump(results, f, indent=4)\n",
        "\n",
        "        # Save to SQLite\n",
        "        database_path = 'healthcare_data.db'\n",
        "        for dataset_name, df in datasets.items():\n",
        "            save_to_sqlite(df, dataset_name, database_path)\n",
        "\n",
        "        # Print results summary\n",
        "        print_results_summary(results)\n",
        "\n",
        "        logger.info(\"Data quality validation process completed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main execution: {str(e)}\")\n",
        "        raise\n",
        "    finally:\n",
        "        if 'spark' in locals():\n",
        "            spark.stop()\n",
        "            logger.info(\"Spark session stopped\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ0fnDVb526G",
        "outputId": "df3972ba-16ec-4aba-ab59-819fde81fcdc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DATA QUALITY VALIDATION SUMMARY ===\n",
            "\n",
            "Dataset Metrics:\n",
            "+------------+--------------+---------+\n",
            "| Dataset    | Metric       |   Value |\n",
            "+============+==============+=========+\n",
            "| patients   | record_count |     974 |\n",
            "+------------+--------------+---------+\n",
            "| patients   | column_count |      20 |\n",
            "+------------+--------------+---------+\n",
            "| encounters | record_count |   27891 |\n",
            "+------------+--------------+---------+\n",
            "| encounters | column_count |      14 |\n",
            "+------------+--------------+---------+\n",
            "| procedures | record_count |   47701 |\n",
            "+------------+--------------+---------+\n",
            "| procedures | column_count |       9 |\n",
            "+------------+--------------+---------+\n",
            "\n",
            "Analysis Results:\n",
            "+------------+-------------------------+------------+\n",
            "| Dataset    | Analyzer                | Value      |\n",
            "+============+=========================+============+\n",
            "| patients   | Record Count            | 974.00     |\n",
            "+------------+-------------------------+------------+\n",
            "| patients   | Gender Distribution (F) | 49.28%     |\n",
            "+------------+-------------------------+------------+\n",
            "| patients   | Gender Distribution (M) | 50.72%     |\n",
            "+------------+-------------------------+------------+\n",
            "| encounters | Record Count            | 27,891.00  |\n",
            "+------------+-------------------------+------------+\n",
            "| encounters | Min Encounter Cost      | 85.55      |\n",
            "+------------+-------------------------+------------+\n",
            "| encounters | Max Encounter Cost      | 146.18     |\n",
            "+------------+-------------------------+------------+\n",
            "| encounters | Mean Encounter Cost     | 116.18     |\n",
            "+------------+-------------------------+------------+\n",
            "| procedures | Record Count            | 47,701.00  |\n",
            "+------------+-------------------------+------------+\n",
            "| procedures | Min Procedure Cost      | 100.00%    |\n",
            "+------------+-------------------------+------------+\n",
            "| procedures | Max Procedure Cost      | 289,531.00 |\n",
            "+------------+-------------------------+------------+\n",
            "| procedures | Mean Procedure Cost     | 2,212.06   |\n",
            "+------------+-------------------------+------------+\n",
            "\n",
            "Verification Results:\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------+\n",
            "| Dataset    | Check Description                                                                                                                                                         | Status   | Details                                                                           |\n",
            "+============+===========================================================================================================================================================================+==========+===================================================================================+\n",
            "| patients   | SizeConstraint(Size(None))                                                                                                                                                | Success  | Dataset contains 974 records, which meets the size requirement.                   |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------+\n",
            "| patients   | CompletenessConstraint(Completeness(Id,None,None))                                                                                                                        | Success  | Column 'Id' is 100.00% complete.                                                  |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------+\n",
            "| patients   | ComplianceConstraint(Compliance(GENDER contained in M,F,`GENDER` IS NULL OR `GENDER` IN ('M','F'),None,List(GENDER),None))                                                | Success  | Gender distribution: F: 480, M: 494                                               |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------+\n",
            "| patients   | CompletenessConstraint(Completeness(BIRTHDATE,None,None))                                                                                                                 | Success  | Column 'BIRTHDATE' is 100.00% complete.                                           |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------+\n",
            "| encounters | SizeConstraint(Size(None))                                                                                                                                                | Success  | Dataset contains 27,891 records, which meets the size requirement.                |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------+\n",
            "| encounters | CompletenessConstraint(Completeness(PATIENT,None,None))                                                                                                                   | Success  | Column 'PATIENT' is 100.00% complete.                                             |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------+\n",
            "| encounters | ComplianceConstraint(Compliance(BASE_ENCOUNTER_COST is non-negative,COALESCE(CAST(BASE_ENCOUNTER_COST AS DECIMAL(20,10)), 0.0) >= 0,None,List(BASE_ENCOUNTER_COST),None)) | Success  | All encounter costs are non-negative. Min: $85.55, Max: $146.18, Mean: $116.18    |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------+\n",
            "| encounters | CompletenessConstraint(Completeness(START,None,None))                                                                                                                     | Success  | Column 'START' is 100.00% complete.                                               |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------+\n",
            "| encounters | CompletenessConstraint(Completeness(STOP,None,None))                                                                                                                      | Success  | Column 'STOP' is 100.00% complete.                                                |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------+\n",
            "| procedures | SizeConstraint(Size(None))                                                                                                                                                | Success  | Dataset contains 47,701 records, which meets the size requirement.                |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------+\n",
            "| procedures | CompletenessConstraint(Completeness(PATIENT,None,None))                                                                                                                   | Success  | Column 'PATIENT' is 100.00% complete.                                             |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------+\n",
            "| procedures | ComplianceConstraint(Compliance(BASE_COST is non-negative,COALESCE(CAST(BASE_COST AS DECIMAL(20,10)), 0.0) >= 0,None,List(BASE_COST),None))                               | Success  | All procedure costs are non-negative. Min: $1.00, Max: $289531.00, Mean: $2212.06 |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------+\n",
            "| procedures | CompletenessConstraint(Completeness(DESCRIPTION,None,None))                                                                                                               | Success  | Column 'DESCRIPTION' is 100.00% complete.                                         |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------+\n",
            "| procedures | CompletenessConstraint(Completeness(START,None,None))                                                                                                                     | Success  | Column 'START' is 100.00% complete.                                               |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------+\n",
            "| procedures | CompletenessConstraint(Completeness(STOP,None,None))                                                                                                                      | Success  | Column 'STOP' is 100.00% complete.                                                |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    }
  ]
}