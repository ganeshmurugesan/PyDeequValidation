{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jryJaNsP24FR",
        "outputId": "a279be84-d790-40aa-b4be-af2259901768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Connecting to security.ubuntu.com (185.125.190.81)] [Connected to cloud.r-project.org (3.161.136\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,526 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,939 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,317 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,661 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,698 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,230 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,610 kB]\n",
            "Fetched 21.4 MB in 4s (6,053 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "--2025-02-24 02:49:52--  https://downloads.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.208.237, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400879762 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.4-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.4-bin-had 100%[===================>] 382.31M  25.6MB/s    in 16s     \n",
            "\n",
            "2025-02-24 02:50:08 (24.3 MB/s) - ‘spark-3.5.4-bin-hadoop3.tgz’ saved [400879762/400879762]\n",
            "\n",
            "Collecting pyspark==3.5.0\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pydeequ\n",
            "  Downloading pydeequ-1.4.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting faker\n",
            "  Downloading Faker-36.1.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark==3.5.0) (0.10.9.7)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from pydeequ) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pydeequ-1.4.0-py3-none-any.whl (37 kB)\n",
            "Downloading Faker-36.1.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425346 sha256=ab1fe751d04a12666028aaa2c01e9e611bee75bb736c777c07a5e937aac9d34c\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/df/61/8c121f50c3cffd77f8178180dd232d90b3b99d1bd61fb6d6be\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark, faker, pydeequ\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.4\n",
            "    Uninstalling pyspark-3.5.4:\n",
            "      Successfully uninstalled pyspark-3.5.4\n",
            "Successfully installed faker-36.1.1 pydeequ-1.4.0 pyspark-3.5.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  sqlite3-doc\n",
            "The following NEW packages will be installed:\n",
            "  sqlite3\n",
            "0 upgraded, 1 newly installed, 0 to remove and 33 not upgraded.\n",
            "Need to get 768 kB of archives.\n",
            "After this operation, 1,873 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.3 [768 kB]\n",
            "Fetched 768 kB in 0s (2,517 kB/s)\n",
            "Selecting previously unselected package sqlite3.\n",
            "(Reading database ... 124926 files and directories currently installed.)\n",
            "Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking sqlite3 (3.37.2-2ubuntu0.3) ...\n",
            "Setting up sqlite3 (3.37.2-2ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "--2025-02-24 02:51:15--  https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.42.0.0/sqlite-jdbc-3.42.0.0.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13030515 (12M) [application/java-archive]\n",
            "Saving to: ‘/content/sqlite-jdbc-3.42.0.0.jar’\n",
            "\n",
            "/content/sqlite-jdb 100%[===================>]  12.43M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-02-24 02:51:15 (92.5 MB/s) - ‘/content/sqlite-jdbc-3.42.0.0.jar’ saved [13030515/13030515]\n",
            "\n",
            "--2025-02-24 02:51:15--  https://repo1.maven.org/maven2/com/amazon/deequ/deequ/2.0.7-spark-3.5/deequ-2.0.7-spark-3.5.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1538311 (1.5M) [application/java-archive]\n",
            "Saving to: ‘/content/deequ-2.0.7-spark-3.5.jar’\n",
            "\n",
            "/content/deequ-2.0. 100%[===================>]   1.47M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-02-24 02:51:15 (21.7 MB/s) - ‘/content/deequ-2.0.7-spark-3.5.jar’ saved [1538311/1538311]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install Java 11\n",
        "!apt-get update\n",
        "!apt-get install -y openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download Spark 3.5.4 with Hadoop 3.3\n",
        "!wget https://downloads.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "# Set environment variables for Java and Spark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\"\n",
        "os.environ[\"SPARK_VERSION\"] = \"3.5\"\n",
        "\n",
        "# Install PySpark 3.5.0, PyDeequ, Pandas and SQLite\n",
        "!pip install pyspark==3.5.0 pydeequ pandas faker\n",
        "!apt-get install sqlite3\n",
        "\n",
        "# Download SQLite JDBC driver\n",
        "!wget https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.42.0.0/sqlite-jdbc-3.42.0.0.jar -O /content/sqlite-jdbc-3.42.0.0.jar\n",
        "\n",
        "# Download Deequ 2.0.7 JAR for Spark 3.5\n",
        "!wget https://repo1.maven.org/maven2/com/amazon/deequ/deequ/2.0.7-spark-3.5/deequ-2.0.7-spark-3.5.jar -O /content/deequ-2.0.7-spark-3.5.jar\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabulate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBHapAcD3JZU",
        "outputId": "fc9d4fb6-99eb-4200-c18e-851352dab318"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import mean, min, max, col, count\n",
        "from pydeequ.analyzers import *\n",
        "from pydeequ.checks import *\n",
        "from pydeequ.verification import *\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import json\n",
        "from tabulate import tabulate\n",
        "import sqlite3\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('data_quality_validation.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def create_spark_session():\n",
        "    \"\"\"Initialize Spark session with required configurations\"\"\"\n",
        "    logger.info(\"Initializing Spark session...\")\n",
        "    try:\n",
        "        spark = (SparkSession.builder\n",
        "            .config(\"spark.jars\", \"/content/sqlite-jdbc-3.42.0.0.jar,/content/deequ-2.0.7-spark-3.5.jar\")\n",
        "            .getOrCreate())\n",
        "        logger.info(\"Spark session created successfully\")\n",
        "        return spark\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to create Spark session: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def load_dataframes(spark):\n",
        "    \"\"\"Load CSV files into Spark DataFrames with error handling\"\"\"\n",
        "    logger.info(\"Loading data from CSV files...\")\n",
        "    try:\n",
        "        patients_df = spark.read.csv('patients.csv', header=True, inferSchema=True)\n",
        "        encounters_df = spark.read.csv('encounters.csv', header=True, inferSchema=True)\n",
        "        procedures_df = spark.read.csv('procedures.csv', header=True, inferSchema=True)\n",
        "\n",
        "        # Log basic statistics\n",
        "        logger.info(f\"Patients dataset loaded: {patients_df.count()} records\")\n",
        "        logger.info(f\"Encounters dataset loaded: {encounters_df.count()} records\")\n",
        "        logger.info(f\"Procedures dataset loaded: {procedures_df.count()} records\")\n",
        "\n",
        "        return patients_df, encounters_df, procedures_df\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def analyze_dataset(spark, df, dataset_name):\n",
        "    \"\"\"Analyze a single dataset\"\"\"\n",
        "    logger.info(f\"Analyzing {dataset_name} dataset...\")\n",
        "    try:\n",
        "        analysis_results = []\n",
        "\n",
        "        # Basic dataset metrics\n",
        "        row_count = df.count()\n",
        "        analysis_results.append({\n",
        "            \"analyzer\": \"Record Count\",\n",
        "            \"value\": row_count\n",
        "        })\n",
        "\n",
        "        # Column completeness analysis\n",
        "        for column in df.columns:\n",
        "            completeness = df.filter(df[column].isNotNull()).count() / float(row_count)\n",
        "            analysis_results.append({\n",
        "                \"analyzer\": f\"Completeness ({column})\",\n",
        "                \"value\": completeness\n",
        "            })\n",
        "\n",
        "        # Dataset-specific analysis\n",
        "        if dataset_name == \"patients\":\n",
        "            # Gender distribution\n",
        "            gender_dist = df.groupBy(\"GENDER\").count().collect()\n",
        "            for row in gender_dist:\n",
        "                analysis_results.append({\n",
        "                    \"analyzer\": f\"Gender Distribution ({row['GENDER']})\",\n",
        "                    \"value\": row['count'] / float(row_count)\n",
        "                })\n",
        "\n",
        "        elif dataset_name == \"encounters\":\n",
        "            # Cost statistics\n",
        "            cost_stats = df.select(\n",
        "                mean(\"BASE_ENCOUNTER_COST\").alias(\"mean\"),\n",
        "                min(\"BASE_ENCOUNTER_COST\").alias(\"min\"),\n",
        "                max(\"BASE_ENCOUNTER_COST\").alias(\"max\")\n",
        "            ).collect()[0]\n",
        "\n",
        "            analysis_results.append({\n",
        "                \"analyzer\": \"Mean Encounter Cost\",\n",
        "                \"value\": float(cost_stats[\"mean\"])\n",
        "            })\n",
        "            analysis_results.append({\n",
        "                \"analyzer\": \"Min Encounter Cost\",\n",
        "                \"value\": float(cost_stats[\"min\"])\n",
        "            })\n",
        "            analysis_results.append({\n",
        "                \"analyzer\": \"Max Encounter Cost\",\n",
        "                \"value\": float(cost_stats[\"max\"])\n",
        "            })\n",
        "\n",
        "        elif dataset_name == \"procedures\":\n",
        "            # Cost statistics\n",
        "            cost_stats = df.select(\n",
        "                mean(\"BASE_COST\").alias(\"mean\"),\n",
        "                min(\"BASE_COST\").alias(\"min\"),\n",
        "                max(\"BASE_COST\").alias(\"max\")\n",
        "            ).collect()[0]\n",
        "\n",
        "            analysis_results.append({\n",
        "                \"analyzer\": \"Mean Procedure Cost\",\n",
        "                \"value\": float(cost_stats[\"mean\"])\n",
        "            })\n",
        "            analysis_results.append({\n",
        "                \"analyzer\": \"Min Procedure Cost\",\n",
        "                \"value\": float(cost_stats[\"min\"])\n",
        "            })\n",
        "            analysis_results.append({\n",
        "                \"analyzer\": \"Max Procedure Cost\",\n",
        "                \"value\": float(cost_stats[\"max\"])\n",
        "            })\n",
        "\n",
        "        return analysis_results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error analyzing {dataset_name} dataset: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def verify_dataset(spark, df, dataset_name):\n",
        "    \"\"\"Run verification checks on a dataset\"\"\"\n",
        "    logger.info(f\"Verifying {dataset_name} dataset...\")\n",
        "    try:\n",
        "        verification_results = []\n",
        "\n",
        "        # Dataset-specific ID field checks\n",
        "        if dataset_name == \"patients\":\n",
        "            id_field = \"Id\"\n",
        "        elif dataset_name in [\"encounters\", \"procedures\"]:\n",
        "            id_field = \"PATIENT\"\n",
        "\n",
        "        # Common checks using the appropriate ID field\n",
        "        id_completeness = df.filter(df[id_field].isNotNull()).count() / float(df.count())\n",
        "        verification_results.append({\n",
        "            \"check_description\": f\"{id_field} Completeness Check\",\n",
        "            \"status\": \"Success\" if id_completeness >= 0.9 else \"Error\",\n",
        "            \"details\": f\"{id_field} completeness: {id_completeness:.2%}\"\n",
        "        })\n",
        "\n",
        "        # Dataset-specific checks\n",
        "        if dataset_name == \"patients\":\n",
        "            # Gender validation\n",
        "            valid_gender = df.filter(df[\"GENDER\"].isin([\"M\", \"F\"])).count() / float(df.count())\n",
        "            verification_results.append({\n",
        "                \"check_description\": \"Gender Format Check\",\n",
        "                \"status\": \"Success\" if valid_gender == 1.0 else \"Error\",\n",
        "                \"details\": f\"Valid gender formats: {valid_gender:.2%}\"\n",
        "            })\n",
        "\n",
        "            # Birthdate presence\n",
        "            birthdate_completeness = df.filter(df[\"BIRTHDATE\"].isNotNull()).count() / float(df.count())\n",
        "            verification_results.append({\n",
        "                \"check_description\": \"Birthdate Completeness Check\",\n",
        "                \"status\": \"Success\" if birthdate_completeness >= 0.9 else \"Error\",\n",
        "                \"details\": f\"Birthdate completeness: {birthdate_completeness:.2%}\"\n",
        "            })\n",
        "\n",
        "        elif dataset_name == \"encounters\":\n",
        "            # Cost validation\n",
        "            valid_costs = df.filter(df[\"BASE_ENCOUNTER_COST\"] >= 0).count() / float(df.count())\n",
        "            verification_results.append({\n",
        "                \"check_description\": \"Encounter Cost Validation\",\n",
        "                \"status\": \"Success\" if valid_costs == 1.0 else \"Error\",\n",
        "                \"details\": f\"Valid costs (non-negative): {valid_costs:.2%}\"\n",
        "            })\n",
        "\n",
        "            # Date completeness\n",
        "            date_completeness = df.filter(\n",
        "                df[\"START\"].isNotNull() & df[\"STOP\"].isNotNull()\n",
        "            ).count() / float(df.count())\n",
        "            verification_results.append({\n",
        "                \"check_description\": \"Date Completeness Check\",\n",
        "                \"status\": \"Success\" if date_completeness >= 0.9 else \"Error\",\n",
        "                \"details\": f\"Date completeness: {date_completeness:.2%}\"\n",
        "            })\n",
        "\n",
        "        elif dataset_name == \"procedures\":\n",
        "            # Cost validation\n",
        "            valid_costs = df.filter(df[\"BASE_COST\"] >= 0).count() / float(df.count())\n",
        "            verification_results.append({\n",
        "                \"check_description\": \"Procedure Cost Validation\",\n",
        "                \"status\": \"Success\" if valid_costs == 1.0 else \"Error\",\n",
        "                \"details\": f\"Valid costs (non-negative): {valid_costs:.2%}\"\n",
        "            })\n",
        "\n",
        "            # Description completeness\n",
        "            desc_completeness = df.filter(df[\"DESCRIPTION\"].isNotNull()).count() / float(df.count())\n",
        "            verification_results.append({\n",
        "                \"check_description\": \"Description Completeness Check\",\n",
        "                \"status\": \"Success\" if desc_completeness >= 0.9 else \"Error\",\n",
        "                \"details\": f\"Description completeness: {desc_completeness:.2%}\"\n",
        "            })\n",
        "\n",
        "            # Date completeness\n",
        "            date_completeness = df.filter(\n",
        "                df[\"START\"].isNotNull() & df[\"STOP\"].isNotNull()\n",
        "            ).count() / float(df.count())\n",
        "            verification_results.append({\n",
        "                \"check_description\": \"Date Completeness Check\",\n",
        "                \"status\": \"Success\" if date_completeness >= 0.9 else \"Error\",\n",
        "                \"details\": f\"Date completeness: {date_completeness:.2%}\"\n",
        "            })\n",
        "\n",
        "        return verification_results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error verifying {dataset_name} dataset: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def save_to_sqlite(df, table_name, db_path):\n",
        "    \"\"\"Save DataFrame to SQLite with logging\"\"\"\n",
        "    try:\n",
        "        pandas_df = df.toPandas()\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        pandas_df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
        "        conn.close()\n",
        "        logger.info(f\"Successfully saved {table_name} to SQLite\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving {table_name} to SQLite: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def print_results_summary(results):\n",
        "    \"\"\"Print formatted results summary\"\"\"\n",
        "    print(\"\\n=== DATA QUALITY VALIDATION SUMMARY ===\\n\")\n",
        "\n",
        "    # Print dataset metrics\n",
        "    print(\"Dataset Metrics:\")\n",
        "    metrics_table = []\n",
        "    for dataset, metrics in results[\"dataset_metrics\"].items():\n",
        "        for metric_name, value in metrics.items():\n",
        "            metrics_table.append([dataset, metric_name, value])\n",
        "    print(tabulate(metrics_table,\n",
        "                  headers=[\"Dataset\", \"Metric\", \"Value\"],\n",
        "                  tablefmt=\"grid\"))\n",
        "\n",
        "    # Print analysis results\n",
        "    print(\"\\nAnalysis Results:\")\n",
        "    analysis_table = []\n",
        "    for dataset, analyses in results[\"analysis_results\"].items():\n",
        "        for analysis in analyses:\n",
        "            value = analysis['value']\n",
        "            formatted_value = f\"{value:.2%}\" if isinstance(value, float) and value <= 1.0 else f\"{value:,.2f}\"\n",
        "            analysis_table.append([\n",
        "                dataset,\n",
        "                analysis[\"analyzer\"],\n",
        "                formatted_value\n",
        "            ])\n",
        "    print(tabulate(analysis_table,\n",
        "                  headers=[\"Dataset\", \"Analyzer\", \"Value\"],\n",
        "                  tablefmt=\"grid\"))\n",
        "\n",
        "    # Print verification results\n",
        "    print(\"\\nVerification Results:\")\n",
        "    verification_table = []\n",
        "    for dataset, verifications in results[\"verification_results\"].items():\n",
        "        for verification in verifications:\n",
        "            verification_table.append([\n",
        "                dataset,\n",
        "                verification[\"check_description\"],\n",
        "                verification[\"status\"],\n",
        "                verification[\"details\"]\n",
        "            ])\n",
        "    print(tabulate(verification_table,\n",
        "                  headers=[\"Dataset\", \"Check Description\", \"Status\", \"Details\"],\n",
        "                  tablefmt=\"grid\"))\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    try:\n",
        "        logger.info(\"Starting data quality validation process\")\n",
        "\n",
        "        # Initialize Spark\n",
        "        spark = create_spark_session()\n",
        "\n",
        "        # Load data\n",
        "        patients_df, encounters_df, procedures_df = load_dataframes(spark)\n",
        "\n",
        "        # Initialize results dictionary\n",
        "        results = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"dataset_metrics\": {},\n",
        "            \"analysis_results\": {},\n",
        "            \"verification_results\": {}\n",
        "        }\n",
        "\n",
        "        # Process each dataset\n",
        "        datasets = {\n",
        "            \"patients\": patients_df,\n",
        "            \"encounters\": encounters_df,\n",
        "            \"procedures\": procedures_df\n",
        "        }\n",
        "\n",
        "        for dataset_name, df in datasets.items():\n",
        "            # Add basic metrics\n",
        "            results[\"dataset_metrics\"][dataset_name] = {\n",
        "                \"record_count\": df.count(),\n",
        "                \"column_count\": len(df.columns)\n",
        "            }\n",
        "\n",
        "            # Run analysis\n",
        "            results[\"analysis_results\"][dataset_name] = analyze_dataset(spark, df, dataset_name)\n",
        "\n",
        "            # Run verification\n",
        "            results[\"verification_results\"][dataset_name] = verify_dataset(spark, df, dataset_name)\n",
        "\n",
        "        # Save results to JSON\n",
        "        with open('data_quality_results.json', 'w') as f:\n",
        "            json.dump(results, f, indent=4)\n",
        "\n",
        "        # Save to SQLite\n",
        "        database_path = 'healthcare_data.db'\n",
        "        for dataset_name, df in datasets.items():\n",
        "            save_to_sqlite(df, dataset_name, database_path)\n",
        "\n",
        "        # Print results summary\n",
        "        print_results_summary(results)\n",
        "\n",
        "        logger.info(\"Data quality validation process completed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main execution: {str(e)}\")\n",
        "        raise\n",
        "    finally:\n",
        "        if 'spark' in locals():\n",
        "            spark.stop()\n",
        "            logger.info(\"Spark session stopped\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ0fnDVb526G",
        "outputId": "16699a83-698c-421b-9b3e-8b96c94b99a0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DATA QUALITY VALIDATION SUMMARY ===\n",
            "\n",
            "Dataset Metrics:\n",
            "+------------+--------------+---------+\n",
            "| Dataset    | Metric       |   Value |\n",
            "+============+==============+=========+\n",
            "| patients   | record_count |     974 |\n",
            "+------------+--------------+---------+\n",
            "| patients   | column_count |      20 |\n",
            "+------------+--------------+---------+\n",
            "| encounters | record_count |   27891 |\n",
            "+------------+--------------+---------+\n",
            "| encounters | column_count |      14 |\n",
            "+------------+--------------+---------+\n",
            "| procedures | record_count |   47701 |\n",
            "+------------+--------------+---------+\n",
            "| procedures | column_count |       9 |\n",
            "+------------+--------------+---------+\n",
            "\n",
            "Analysis Results:\n",
            "+------------+------------------------------------+------------+\n",
            "| Dataset    | Analyzer                           | Value      |\n",
            "+============+====================================+============+\n",
            "| patients   | Record Count                       | 974.00     |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (Id)                  | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (BIRTHDATE)           | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (DEATHDATE)           | 15.81%     |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (PREFIX)              | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (FIRST)               | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (LAST)                | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (SUFFIX)              | 2.16%      |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (MAIDEN)              | 39.63%     |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (MARITAL)             | 99.90%     |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (RACE)                | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (ETHNICITY)           | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (GENDER)              | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (BIRTHPLACE)          | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (ADDRESS)             | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (CITY)                | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (STATE)               | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (COUNTY)              | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (ZIP)                 | 85.42%     |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (LAT)                 | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Completeness (LON)                 | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Gender Distribution (F)            | 49.28%     |\n",
            "+------------+------------------------------------+------------+\n",
            "| patients   | Gender Distribution (M)            | 50.72%     |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Record Count                       | 27,891.00  |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Completeness (Id)                  | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Completeness (START)               | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Completeness (STOP)                | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Completeness (PATIENT)             | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Completeness (ORGANIZATION)        | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Completeness (PAYER)               | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Completeness (ENCOUNTERCLASS)      | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Completeness (CODE)                | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Completeness (DESCRIPTION)         | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Completeness (BASE_ENCOUNTER_COST) | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Completeness (TOTAL_CLAIM_COST)    | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Completeness (PAYER_COVERAGE)      | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Completeness (REASONCODE)          | 29.94%     |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Completeness (REASONDESCRIPTION)   | 29.94%     |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Mean Encounter Cost                | 116.18     |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Min Encounter Cost                 | 85.55      |\n",
            "+------------+------------------------------------+------------+\n",
            "| encounters | Max Encounter Cost                 | 146.18     |\n",
            "+------------+------------------------------------+------------+\n",
            "| procedures | Record Count                       | 47,701.00  |\n",
            "+------------+------------------------------------+------------+\n",
            "| procedures | Completeness (START)               | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| procedures | Completeness (STOP)                | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| procedures | Completeness (PATIENT)             | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| procedures | Completeness (ENCOUNTER)           | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| procedures | Completeness (CODE)                | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| procedures | Completeness (DESCRIPTION)         | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| procedures | Completeness (BASE_COST)           | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| procedures | Completeness (REASONCODE)          | 22.55%     |\n",
            "+------------+------------------------------------+------------+\n",
            "| procedures | Completeness (REASONDESCRIPTION)   | 22.55%     |\n",
            "+------------+------------------------------------+------------+\n",
            "| procedures | Mean Procedure Cost                | 2,212.06   |\n",
            "+------------+------------------------------------+------------+\n",
            "| procedures | Min Procedure Cost                 | 100.00%    |\n",
            "+------------+------------------------------------+------------+\n",
            "| procedures | Max Procedure Cost                 | 289,531.00 |\n",
            "+------------+------------------------------------+------------+\n",
            "\n",
            "Verification Results:\n",
            "+------------+--------------------------------+----------+-------------------------------------+\n",
            "| Dataset    | Check Description              | Status   | Details                             |\n",
            "+============+================================+==========+=====================================+\n",
            "| patients   | Id Completeness Check          | Success  | Id completeness: 100.00%            |\n",
            "+------------+--------------------------------+----------+-------------------------------------+\n",
            "| patients   | Gender Format Check            | Success  | Valid gender formats: 100.00%       |\n",
            "+------------+--------------------------------+----------+-------------------------------------+\n",
            "| patients   | Birthdate Completeness Check   | Success  | Birthdate completeness: 100.00%     |\n",
            "+------------+--------------------------------+----------+-------------------------------------+\n",
            "| encounters | PATIENT Completeness Check     | Success  | PATIENT completeness: 100.00%       |\n",
            "+------------+--------------------------------+----------+-------------------------------------+\n",
            "| encounters | Encounter Cost Validation      | Success  | Valid costs (non-negative): 100.00% |\n",
            "+------------+--------------------------------+----------+-------------------------------------+\n",
            "| encounters | Date Completeness Check        | Success  | Date completeness: 100.00%          |\n",
            "+------------+--------------------------------+----------+-------------------------------------+\n",
            "| procedures | PATIENT Completeness Check     | Success  | PATIENT completeness: 100.00%       |\n",
            "+------------+--------------------------------+----------+-------------------------------------+\n",
            "| procedures | Procedure Cost Validation      | Success  | Valid costs (non-negative): 100.00% |\n",
            "+------------+--------------------------------+----------+-------------------------------------+\n",
            "| procedures | Description Completeness Check | Success  | Description completeness: 100.00%   |\n",
            "+------------+--------------------------------+----------+-------------------------------------+\n",
            "| procedures | Date Completeness Check        | Success  | Date completeness: 100.00%          |\n",
            "+------------+--------------------------------+----------+-------------------------------------+\n"
          ]
        }
      ]
    }
  ]
}